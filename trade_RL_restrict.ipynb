{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a custom Environment for Financial Trading\n",
    "\n",
    "Some examples on the market\n",
    "* [custom env example](https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb#scrollTo=RqxatIwPOXe_)\n",
    "* [StockTradingEnv by Adam King](https://github.com/notadamking/Stock-Trading-Environment)\n",
    "* [FinRL](https://github.com/AI4Finance-Foundation/FinRL)\n",
    "\n",
    "Target is to construct a custom Env for pair trading\n",
    "\n",
    "This env restrict the behaviour of RL learner to pair trading only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from utils.read2df import read2df\n",
    "\n",
    "os.makedirs(\"result/rl-restrict\", exist_ok=True)\n",
    "strategy_txt = f\"result/rl-restrict/strategy.txt\"\n",
    "os.remove(strategy_txt) if os.path.exists(strategy_txt) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['BTCUSDT', 'ETHUSDT', 'LTCUSDT', 'XMRUSDT', 'BNBUSDT', 'ADAUSDT', 'DOGEUSDT', 'SOLUSDT', 'TRXUSDT']\n",
    "\n",
    "start_date = '2022-01-01'\n",
    "trade_date = '2023-01-01'\n",
    "\n",
    "# freqs = {'1h':60, '2h':120, '4h':240, '6h':360, '8h':480, '12h':720, '1d':1440}\n",
    "freqs = {'3m':3, '5m':5, '15m':15, '30m':30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from `binance-public-data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if symbols is None:\n",
    "    !python binance-public-data/python/download-kline.py -i {\" \".join(list(freqs.keys()))} -startDate {start_date} -t spot -skip-daily 1\n",
    "else:\n",
    "    !python binance-public-data/python/download-kline.py -s {\" \".join(symbols)} -i {\" \".join(list(freqs.keys()))} -startDate {start_date} -t spot -skip-daily 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = read2df(symbols, freqs)\n",
    "dfs = read2df(symbols, freqs)\n",
    "\n",
    "df0 = dfs[0][dfs[0]['tic']=='BTCUSDT'].reset_index(drop=True)\n",
    "df1 = dfs[0][dfs[0]['tic']=='ETHUSDT'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data before `trade_data` as training data, after `trade_data` is trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = df0[df0['datetime'] < trade_date]\n",
    "train1 = df1[df1['datetime'] < trade_date]\n",
    "\n",
    "test0 = df0[df0['datetime'] >= trade_date]\n",
    "test1 = df1[df1['datetime'] >= trade_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass PairTradingActionSpace(gym.Space):\\n  def __init__(self, low=-1.0, high=1.0, shape=(2, ), dtype=np.float32):\\n    super().__init__(shape, dtype)\\n    self.low = low\\n    self.high = high\\n\\n  def sample(self):\\n    action = np.random.uniform(self.low, self.high, self.shape)\\n    # Normalize the action so that the summation of action[0] and action[1] is within -1 and 1.\\n    action = action / np.linalg.norm(action)\\n    return action\\n\\n  def contains(self, x):\\n    return np.all(self.low <= x) and np.all(x <= self.high) and np.linalg.norm(x) <= 1.0\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't use custom observation & action spaces\n",
    "# See the warning on https://gymnasium.farama.org/api/spaces/\n",
    "\n",
    "'''\n",
    "class PairTradingActionSpace(gym.Space):\n",
    "  def __init__(self, low=-1.0, high=1.0, shape=(2, ), dtype=np.float32):\n",
    "    super().__init__(shape, dtype)\n",
    "    self.low = low\n",
    "    self.high = high\n",
    "\n",
    "  def sample(self):\n",
    "    action = np.random.uniform(self.low, self.high, self.shape)\n",
    "    # Normalize the action so that the summation of action[0] and action[1] is within -1 and 1.\n",
    "    action = action / np.linalg.norm(action)\n",
    "    return action\n",
    "\n",
    "  def contains(self, x):\n",
    "    return np.all(self.low <= x) and np.all(x <= self.high) and np.linalg.norm(x) <= 1.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the custom Environment\n",
    "\n",
    "The behaviour of RL learner is restricted. \n",
    "\n",
    "The action is defined as discrete actions -1, 0, 1\n",
    "\n",
    "-1 means short df0 long df1, 0 means close position, +1 means long df0 short df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lookback period for the observation space\n",
    "PERIOD = 100 # Only look at the current price\n",
    "CASH = 10000\n",
    "\n",
    "class PairTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    # for pair trading, we need to feed in two OHLCV dataframes\n",
    "    def __init__(self, df0, df1, tc=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        if not df0['time'].equals(df1['time']):\n",
    "            raise ValueError(\"Two dataframe must have same time index\")\n",
    "\n",
    "        self.tic0 = df0['tic'].iloc[0]\n",
    "        self.tic1 = df1['tic'].iloc[0]\n",
    "\n",
    "        # transaction cost\n",
    "        self.tc = tc\n",
    "\n",
    "        self.df0 = df0['close']\n",
    "        self.df1 = df1['close']\n",
    "\n",
    "        self.reward_range = (-np.inf, np.inf)\n",
    "\n",
    "        self.action_space = spaces.Dict({\n",
    "            \"isAction\": spaces.Discrete(2), # Whether we make an action or not (0 or 1)\n",
    "            \"amount\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float64)  # Long max 100% or short max 100%\n",
    "        })\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"isPosition\": spaces.Discrete(2),  # Whether we have a Position (0 or 1)\n",
    "            \"zscore\": spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float64)  # Zscore of the spread\n",
    "        })\n",
    "\n",
    "        # if the length is 35, then the index shall be 0~34\n",
    "        self.max_steps = len(df0)-1\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # The current step is always higher than the PERIOD as defined in the \n",
    "\n",
    "        # check if in position\n",
    "        self.isPosition = int(self.holding0 == self.holding1 == 0)\n",
    "\n",
    "        model = sm.OLS(\n",
    "            df1['close'].iloc[self.current_step-PERIOD+1: self.current_step], \n",
    "            sm.add_constant(df0['close'].iloc[self.current_step-PERIOD+1: self.current_step])\n",
    "        ).fit()\n",
    "\n",
    "        residuals = model.resid\n",
    "        zscores = residuals / np.std(residuals)\n",
    "        \n",
    "        return {\"isPosition\": self.isPosition, \"zscore\": np.array([zscores.iloc[-1]])}\n",
    "\n",
    "    def _take_action(self, action):\n",
    "\n",
    "        # See if we need to take an action\n",
    "        if action[\"isAction\"]:\n",
    "            curr_price0 = self.df0['close'].iloc[self.current_step]\n",
    "            curr_price1 = self.df1['close'].iloc[self.current_step]\n",
    "            \n",
    "            if self.isPosition: # We shouldn't do any action if we are already holding the position\n",
    "                self.cash += self.holding0*curr_price0 + self.holding1*curr_price1\n",
    "                self.holding0 = 0\n",
    "                self.holding1 = 0\n",
    "            \n",
    "            else: # That means we don't have any position right now\n",
    "                # evaluate purchasing power \n",
    "                max_amount0 = self.cash/curr_price0\n",
    "                max_amount1 = self.cash/curr_price1\n",
    "\n",
    "                # We allow the action from -1 to 1\n",
    "                self.holding0 = action['amount']*max_amount0\n",
    "                self.holding1 = -action['amount']*max_amount1\n",
    "\n",
    "                # if action is 0, then clear out based on curr_holding*curr_price\n",
    "                # if action is -1, then use all the money to sell df0 and buy df1\n",
    "                # if action is 1, then use all the money to buy df1 and sell df0\n",
    "                self.cash -= self.holding0*curr_price0 + self.holding1*curr_price1\n",
    "\n",
    "            # We record the net_worth from previous period to prev_net_worth\n",
    "            self.prev_net_worth = self.net_worth\n",
    "            self.net_worth = self.cash + self.holding0*curr_price0 + self.holding1*curr_price1\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        self._take_action(action)\n",
    "        self.current_step += 1\n",
    "\n",
    "        observation = self._next_observation()\n",
    "        reward = self.net_worth - self.prev_net_worth\n",
    "        terminated = bool(self.current_step >= self.max_steps)\n",
    "        truncated = bool(self.net_worth <= 0)\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.cash = CASH\n",
    "        self.net_worth = CASH\n",
    "        self.prev_net_worth = CASH\n",
    "        self.max_net_worth = CASH\n",
    "        self.isPosition = 0\n",
    "        self.holding0 = 0\n",
    "        self.holding1 = 0\n",
    "\n",
    "        self.current_step = np.random.randint(PERIOD, self.max_steps)\n",
    "\n",
    "        return self._next_observation(), {}\n",
    "    \n",
    "    def render(self):\n",
    "        profit = self.net_worth - CASH\n",
    "\n",
    "        with open(strategy_txt, \"a\") as f:\n",
    "            f.write(f\"Current net worth is {self.net_worth}, cash is {self.cash}\\n\")\n",
    "            f.write(f\"Actions for this step is {self.action}\\n\")\n",
    "            f.write(f\"Current holding is {self.holding0} of {self.tic0} and {self.holding1} of {self.tic1}\\n\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check with baselin3 `env_checker`\n",
    "\n",
    "Check if the env meets the requirements of `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HSY/miniconda3/envs/uoa-mdt/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:103: UserWarning: The action space is not based off a numpy array. Typically this means it's either a Dict or Tuple space. This type of action space is currently not supported by Stable Baselines 3. You should try to flatten the action using a wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "# > UserWarning: The action space is not based off a numpy array. Typically this means it's either a Dict or Tuple space. This type of action space is currently not supported by Stable Baselines 3. You should try to flatten the action using a wrapper.\n",
    "# Baseline 3 does not support Dict/Tuple action spaces....only Box Discrete MultiDiscrete MultiBinary\n",
    "# Is there another way to achieve the same functionality?\n",
    "\n",
    "env = PairTradingEnv(train0, train1)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a test run with random generated actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space: Dict('isPosition': Discrete(2), 'zscore': Box(-inf, inf, (1,), float64))\n",
      "action_space: Dict('amount': Box(-1.0, 1.0, (1,), float64), 'isAction': Discrete(2))\n",
      "action_space.sample: OrderedDict([('amount', array([0.88960578])), ('isAction', 1)])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStep \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m f\u001b[39m.\u001b[39mclose()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action\u001b[39m=\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m env\u001b[39m.\u001b[39mrender()\n",
      "\u001b[1;32m/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb Cell 16\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_action(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_observation()\n",
      "\u001b[1;32m/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_action\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# See if we need to take an action\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mif\u001b[39;00m action[\u001b[39m\"\u001b[39;49m\u001b[39misAction\u001b[39;49m\u001b[39m\"\u001b[39;49m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         curr_price0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf0[\u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X21sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         curr_price1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf1[\u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "env = PairTradingEnv(train0, train1)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(f\"observation_space: {env.observation_space}\")\n",
    "print(f\"action_space: {env.action_space}\")\n",
    "print(f\"action_space.sample: {env.action_space.sample()}\")\n",
    "\n",
    "n_steps = 5\n",
    "\n",
    "with open(strategy_txt, \"a\") as f:\n",
    "    f.write(f\"Start random testing\\n\")\n",
    "f.close()\n",
    "\n",
    "for step in range(n_steps):\n",
    "    with open(strategy_txt, \"a\") as f:\n",
    "        f.write(f\"Step {step + 1}\\n\")\n",
    "    f.close()\n",
    "    obs, reward, terminated, truncated, info = env.step(action=random.randint(0, 2))\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO model from stable_baselines3\n",
    "\n",
    "Train with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Dict('amount': Box(-1.0, 1.0, (1,), float64), 'isAction': Discrete(2)) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m \u001b[39mimport\u001b[39;00m PPO\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m PairTradingEnv(train0, train1)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, tensorboard_log\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogs\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/HSY/PhD/backtrader-crypto-rl/trade_RL_restrict.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mppo_pairtrading\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/uoa-mdt/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:104\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m ):\n\u001b[0;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    105\u001b[0m         policy,\n\u001b[1;32m    106\u001b[0m         env,\n\u001b[1;32m    107\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    108\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m    109\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    110\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[1;32m    111\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[1;32m    112\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[1;32m    113\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[1;32m    114\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m    115\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m    116\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m    117\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m    118\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m    119\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    120\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    122\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    123\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    124\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[1;32m    125\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[1;32m    126\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[1;32m    127\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[1;32m    128\u001b[0m         ),\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     \u001b[39m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/miniconda3/envs/uoa-mdt/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:81\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     60\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     82\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[1;32m     83\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m     84\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     85\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[1;32m     86\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     87\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     88\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[1;32m     89\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[1;32m     90\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     91\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     92\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[1;32m     93\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[1;32m     94\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/miniconda3/envs/uoa-mdt/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:180\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vec_normalize_env \u001b[39m=\u001b[39m unwrap_vec_normalize(env)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m supported_action_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[1;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe algorithm only supports \u001b[39m\u001b[39m{\u001b[39;00msupported_action_spaces\u001b[39m}\u001b[39;00m\u001b[39m as action spaces \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m}\u001b[39;00m\u001b[39m was provided\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m support_multi_env \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: the model does not support multiple envs; it requires \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma single vectorized environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.box.Box'>, <class 'gymnasium.spaces.discrete.Discrete'>, <class 'gymnasium.spaces.multi_discrete.MultiDiscrete'>, <class 'gymnasium.spaces.multi_binary.MultiBinary'>) as action spaces but Dict('amount': Box(-1.0, 1.0, (1,), float64), 'isAction': Discrete(2)) was provided"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = PairTradingEnv(train0, train1)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0, tensorboard_log=\"logs\")\n",
    "model.learn(total_timesteps=10000)\n",
    "model.save(\"ppo_pairtrading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model on Trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = PPO.load(\"ppo_pairtrading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Finished!\n"
     ]
    }
   ],
   "source": [
    "env = PairTradingEnv(test0, test1)\n",
    "\n",
    "with open(strategy_txt, \"a\") as f:\n",
    "    f.write(f\"Start test trading\\n\")\n",
    "f.close()\n",
    "\n",
    "env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "    if terminated:\n",
    "        print(\"Test Finished!\")\n",
    "        break\n",
    "    elif truncated:\n",
    "        print(\"bankrupted!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uoa-mdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
