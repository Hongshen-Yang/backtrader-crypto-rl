{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a custom Environment for Financial Trading\n",
    "\n",
    ">***Hold it for now, try more restrictive methods***\n",
    "\n",
    "Some examples on the market\n",
    "* [custom env example](https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb#scrollTo=RqxatIwPOXe_)\n",
    "* [StockTradingEnv by Adam King](https://github.com/notadamking/Stock-Trading-Environment)\n",
    "* [FinRL](https://github.com/AI4Finance-Foundation/FinRL)\n",
    "\n",
    "Target is to construct a custom Env for pair trading\n",
    "\n",
    "This env gives the RL learner freedom to operate whatever it wants. Even long n short simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.read2df import read2df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['BTCUSDT', 'ETHUSDT', 'LTCUSDT', 'XMRUSDT', 'BNBUSDT', 'ADAUSDT', 'DOGEUSDT', 'SOLUSDT', 'TRXUSDT']\n",
    "start_date = '2018-01-01'\n",
    "\n",
    "# freqs = {'1h':60, '2h':120, '4h':240, '6h':360, '8h':480, '12h':720, '1d':1440}\n",
    "freqs = {'1m':1, '3m':3, '5m':5, '15m':15, '30m':30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from `binance-public-data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if symbols is None:\n",
    "    !python binance-public-data/python/download-kline.py -i {\" \".join(list(freqs.keys()))} -startDate {start_date} -t spot -skip-daily 1\n",
    "else:\n",
    "    !python binance-public-data/python/download-kline.py -s {\" \".join(symbols)} -i {\" \".join(list(freqs.keys()))} -startDate {start_date} -t spot -skip-daily 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = read2df(symbols, freqs)\n",
    "dfs = read2df(symbols, freqs)\n",
    "\n",
    "df0 = dfs[0][dfs[0]['tic']=='BTCUSDT'].reset_index(drop=True)\n",
    "df1 = dfs[0][dfs[0]['tic']=='ETHUSDT'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data before `trade_data` as training data, after `trade_data` is trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_date = '2023-01-01'\n",
    "\n",
    "train0 = df0[df0['datetime'] < trade_date]\n",
    "train1 = df1[df1['datetime'] < trade_date]\n",
    "\n",
    "trade0 = df0[df0['datetime'] >= trade_date]\n",
    "trade1 = df1[df1['datetime'] >= trade_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use custom observation & action spaces\n",
    "# See the warning on https://gymnasium.farama.org/api/spaces/\n",
    "\n",
    "'''\n",
    "class PairTradingActionSpace(gym.Space):\n",
    "  def __init__(self, low=-1.0, high=1.0, shape=(2, ), dtype=np.float32):\n",
    "    super().__init__(shape, dtype)\n",
    "    self.low = low\n",
    "    self.high = high\n",
    "\n",
    "  def sample(self):\n",
    "    action = np.random.uniform(self.low, self.high, self.shape)\n",
    "    # Normalize the action so that the summation of action[0] and action[1] is within -1 and 1.\n",
    "    action = action / np.linalg.norm(action)\n",
    "    return action\n",
    "\n",
    "  def contains(self, x):\n",
    "    return np.all(self.low <= x) and np.all(x <= self.high) and np.linalg.norm(x) <= 1.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the custom Environment\n",
    "\n",
    "The RL learner can do whatever they want. \n",
    "\n",
    "We want to see if it can learn to be market-neutral itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lookback period for the observation space\n",
    "PERIOD = 1440\n",
    "CASH = 10000\n",
    "\n",
    "class PairTradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    # for pair trading, we need to feed in two OHLCV dataframes\n",
    "    def __init__(self, df0, df1, tc=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        if not df0['time'].equals(df1['time']):\n",
    "            raise ValueError(\"Two dataframe must have same time index\")\n",
    "\n",
    "        self.tic0 = df0['tic'].iloc[0]\n",
    "        self.tic1 = df1['tic'].iloc[0]\n",
    "\n",
    "        # transaction cost\n",
    "        self.tc = tc\n",
    "\n",
    "        # get two datasets\n",
    "        self.df0 = df0[['time', 'open', 'high', 'low', 'close', 'volume']]\n",
    "        self.df1 = df1[['time', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "        self.reward_range = (-np.inf, np.inf)\n",
    "\n",
    "        # -1 means short 100%, 1 means long 100%, 0 means do nothing\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2, ), dtype=np.float32)\n",
    "\n",
    "        # The data requires to be at least [time, open, high, low, close, volume]\n",
    "        # Let's assume that we feed in previous 30 period data into the observation_space\n",
    "        self.observation_space = spaces.Box(low=0.0, high=np.inf, shape=(2*PERIOD*6,), dtype=np.float64)\n",
    "\n",
    "        # if the length is 35, then the index shall be 0~34\n",
    "        self.max_steps = len(df0)-1\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # The current step is always higher than the PERIOD as defined in the \n",
    "\n",
    "        obs_df0 = self.df0.iloc[self.current_step-PERIOD: self.current_step]\n",
    "        obs_df1 = self.df1.iloc[self.current_step-PERIOD: self.current_step]\n",
    "\n",
    "        obs = np.array([obs_df0, obs_df1]).flatten()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        self.action = action\n",
    "\n",
    "        current_price0 = self.df0['close'].iloc[self.current_step]\n",
    "        current_price1 = self.df1['close'].iloc[self.current_step]\n",
    "\n",
    "        # evaluate purchasing power \n",
    "        max_amount0 = self.net_worth/current_price0\n",
    "        max_amount1 = self.net_worth/current_price1\n",
    "\n",
    "        curr_holding0 = self.holding0/max_amount0\n",
    "        curr_holding1 = self.holding1/max_amount1\n",
    "\n",
    "        # clip the action to the summation of [-1, 1]\n",
    "        if sum(self.action) > 1:\n",
    "            action0 = self.action[0]/(sum(self.action)+self.tc)\n",
    "            action1 = self.action[1]/(sum(self.action)+self.tc)\n",
    "            self.action = [action0, action1]\n",
    "        elif sum(self.action) < -1:\n",
    "            action0 = self.action[0]/(sum(self.action)-self.tc)\n",
    "            action1 = self.action[1]/(sum(self.action)-self.tc)\n",
    "\n",
    "        # if curr_h is -70%, action is -40%, then we need to clip the action to -30%\n",
    "        if curr_holding0 + self.action[0] > 1:\n",
    "            self.action[0] = 1 - curr_holding0\n",
    "        elif curr_holding0 + self.action[0] < -1:\n",
    "            self.action[0] = -1 - curr_holding0\n",
    "\n",
    "        if curr_holding1 + self.action[1] > 1:\n",
    "            self.action[1] = 1 - curr_holding1\n",
    "        elif curr_holding0 + self.action[0] < -1:\n",
    "            self.action[1] = -1 - curr_holding1\n",
    "\n",
    "        self.holding0 += self.action[0]*max_amount0\n",
    "        self.holding1 += self.action[1]*max_amount1\n",
    "        self.cash -= self.cash*sum(action)*(1+self.tc)\n",
    "\n",
    "        # We record the net_worth from previous period to prev_net_worth\n",
    "        self.prev_net_worth = self.net_worth\n",
    "        self.net_worth = self.cash + self.holding0*current_price0 + self.holding1*current_price1\n",
    "\n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        self.current_step += 1\n",
    "\n",
    "        observation = self._next_observation()\n",
    "\n",
    "        # TODO: what if I heavily punish loss?\n",
    "        reward = self.net_worth - self.prev_net_worth\n",
    "        \n",
    "        terminated = bool(self.current_step >= self.max_steps)\n",
    "        truncated = bool(self.net_worth <= 0)\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.cash = CASH\n",
    "        self.net_worth = CASH\n",
    "        self.max_net_worth = CASH\n",
    "        self.holding0 = 0\n",
    "        self.holding1 = 0\n",
    "\n",
    "        self.current_step = np.random.randint(PERIOD, self.max_steps)\n",
    "\n",
    "        return self._next_observation(), {}\n",
    "    \n",
    "    def render(self):\n",
    "        profit = self.net_worth - CASH\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Current profit is {profit}, cash is {self.cash}, net worth is {self.net_worth}\")\n",
    "        print(f\"Actions for this step is {self.tic0} for {self.action[0]} and {self.tic1} for {self.action[1]}\")\n",
    "        print(f\"Current holding is {self.holding0} of {self.tic0} and {self.holding1} of {self.tic1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check with baselin3 `env_checker`\n",
    "\n",
    "Check if the env meets the requirements of `stable_baseline3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = PairTradingEnv(train0, train1)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a test run with random generated actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "env = PairTradingEnv(train0, train1)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(f\"observation_space: {env.observation_space}\")\n",
    "print(f\"action_space: {env.action_space}\")\n",
    "print(f\"action_space.sample: {env.action_space.sample()}\")\n",
    "\n",
    "n_steps = 100\n",
    "\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(action=[random.uniform(-1, 1) for _ in range(2)])\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Test Finished!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO model from stable_baselines3\n",
    "\n",
    "Train with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = PairTradingEnv(train0, train1)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "model.save(\"ppo_pairtrading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation from AI\n",
    "\n",
    "---\n",
    "\n",
    "The text you've provided appears to be a summary of metrics and statistics related to some kind of training or optimization process. These values are often generated during the training of machine learning models, particularly reinforcement learning models like PPO (Proximal Policy Optimization) or other similar algorithms. Let's break down what each section means:\n",
    "\n",
    "1. **Rollout**:\n",
    "   - `ep_len_mean`: This is the mean (average) length of episodes. In a reinforcement learning context, episodes are sequences of actions taken by an agent in an environment until a termination condition is met.\n",
    "\n",
    "   - `ep_rew_mean`: This is the mean (average) reward obtained in episodes. It seems to be very negative, which might indicate that the agent is not performing well or that the environment is very challenging.\n",
    "\n",
    "2. **Time**:\n",
    "   - `fps`: Frames per second. This tells you how fast the training is progressing in terms of processing frames or steps in the environment per second.\n",
    "\n",
    "   - `iterations`: The number of training iterations that have been completed.\n",
    "\n",
    "   - `time_elapsed`: The total time (in some unit, possibly seconds) that has elapsed during the training.\n",
    "\n",
    "   - `total_timesteps`: The total number of timesteps or steps the agent has taken in the environment.\n",
    "\n",
    "3. **Train**:\n",
    "   - `approx_kl`: A measure of how much the current policy differs from the previous policy. It's used to control the rate of policy updates.\n",
    "\n",
    "   - `clip_fraction`: The fraction of actions that were clipped during training. Clipping means that the policy update is bounded within a certain range to ensure stability.\n",
    "\n",
    "   - `clip_range`: The range within which the policy update is clipped.\n",
    "\n",
    "   - `entropy_loss`: A measure of the entropy of the policy distribution. It's often used to encourage exploration.\n",
    "\n",
    "   - `explained_variance`: A measure of how well the value function (used to estimate expected rewards) explains the actual rewards. A value of 0 means no explanation, and 1 means perfect explanation.\n",
    "\n",
    "   - `learning_rate`: The learning rate used in the optimization process.\n",
    "\n",
    "   - `loss`: The overall loss of the model. In this case, it's a very large number, which might indicate a problem with the training process.\n",
    "\n",
    "   - `n_updates`: The number of updates performed on the policy.\n",
    "\n",
    "   - `policy_gradient_loss`: A loss term related to the policy gradient method used in reinforcement learning.\n",
    "\n",
    "   - `std`: The standard deviation of the policy distribution.\n",
    "\n",
    "   - `value_loss`: A loss term related to the value function, which estimates expected rewards.\n",
    "\n",
    "These values are crucial for monitoring and understanding the progress of a reinforcement learning training process. To assess whether the training is successful or not, you would typically look at metrics like `ep_rew_mean` (average reward), `approx_kl` (policy stability), and `explained_variance` (how well the model explains rewards). The large values for `loss` and `value_loss` could be indicative of problems in the training process, but further analysis would be needed to diagnose the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model on Trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = PPO.load(\"ppo_pairtrading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PairTradingEnv(trade0, trade1)\n",
    "\n",
    "env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Test Finished!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems we went bankrupcy only after a few steps.\n",
    "\n",
    "That means the learning does not work well if we only feed in historical a rolling frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uoa-mdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
